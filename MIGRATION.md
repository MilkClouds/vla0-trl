# Migration Notes

Technical details on differences between this repo and the [original VLA-0](https://github.com/NVlabs/vla0).

---

## Quick Reference

### Train

| Feature | Original | This Repo | Status |
|---------|----------|-----------|--------|
| Multi-GPU | `mp.spawn` + DDP | `accelerate` | ✅ |
| AMP | `cfg.EXP.AMP` | `SFTConfig.bf16` | ✅ |
| Gradient checkpointing | `cfg.TRAIN.grad_checkpoint` | `SFTConfig.gradient_checkpointing` | ✅ |
| Resume checkpoint | `--resume` | `SFTConfig.resume_from_checkpoint` | ✅ |
| WandB | ❌ | `SFTConfig.report_to="wandb"` | ✅ New |

### Eval

| Feature | Original | This Repo | Status |
|---------|----------|-----------|--------|
| `action_horizon` | 0 → fallback to 8 | 1 | ⚠️ Different |
| `ensemble_prediction` | v1 & v2 | v1 & v2 | ✅ |
| `skip_evaluated` | ✅ | ✅ | ✅ |
| `torch_compile` | `--no-torch-compile` | `--torch_compile` | ✅ |

---

## 1. Training Configuration

### 1.1 DDP and Epochs

The original does not use `DistributedSampler`. Each GPU sees the full dataset per epoch.

| | Original | This Repo |
|--|----------|-----------|
| Sampler | None (shuffle=True) | DistributedSampler |
| Data per GPU per epoch | 100% | 100% / N GPUs |
| Epochs to match | 24 | 24 × 8 = **192** |

### 1.2 Learning Rate

Original scales LR by GPU count:

```python
optimizer = AdamW(params, lr=cfg.TRAIN.lr * num_gpus)  # 5e-6 × 8 = 4e-5
```

This repo uses 4e-5 directly. Without this, training does not converge.

### 1.3 Summary

| Parameter | Original | This Repo |
|-----------|----------|-----------|
| LR | 5e-6 × 8 = 4e-5 | 4e-5 |
| Epochs | 24 | 192 |
| Batch (per GPU) | 8 | 8 |
| Gradient clip | 0.0 | 0.0 |

---

## 2. Eval Behavior

### 2.1 action_horizon

```
horizon = 8        → Model predicts 8 actions
action_horizon = N → Execute N, then re-query
```

| | Default | Behavior |
|--|---------|----------|
| Original | 0 | Falls back to config horizon = 8 |
| This Repo | 1 | Re-query every step |

Use `--action_horizon 8` to match original.

### 2.2 Ensemble Prediction

Averages overlapping action chunks.

```
chunks:  [A,   B,   C  ]  (oldest → newest)
weights: [0.5, 0.5, 1.0]
result = (0.5A + 0.5B + C) / 2 = 0.25A + 0.25B + 0.5C
```

Version 1: All old chunks get flat 0.5 weight.
Version 2: Exponential decay (`weight^(n-i-1)`).

---

## 3. Verified Equivalence

Run: `pytest scripts/verify_migration.py -v`

| Component | Status |
|-----------|--------|
| Action Discretization | ✅ Match |
| Image Tiling | ✅ Match |
| Label Masking | ✅ Match |
| Collator Output | ✅ Match |

### Collator Output

| Metric | Value |
|--------|-------|
| Total tokens | 433 |
| Masked (labels=-100) | 226 |
| Unmasked | 207 |
| Vision tokens | 128 |
| Action values | 56 (8×7) |

**Decoded**:
```
<|im_start|>system
Analyze the input image and predict robot actions for the next 8 timesteps. Each action has 7 dimensions. Output a single sequence of 56 integers (0-1000 each), representing the 8 timesteps sequentially. Provide only space separated numbers. Nothing else.<|im_end|>
<|im_start|>user
Picture 1: <|vision_start|><|image_pad|>...(128 pads)...<|vision_end|>put the white mug on the left plate and put the yellow and white mug on the right plate<|im_end|>
<|im_start|>assistant
474 479 460 669 391 674 0 471 479 460 669 391 674 0 458 479 460 669 391 674 0 455 472 460 669 391 674 0 455 444 460 669 391 674 0 455 396 449 669 391 674 0 449 335 422 669 391 674 0 422 282 397 669 391 674 0<|im_end|>
```

**input_ids (433 tokens)**:
```
[151644, 8948, 198, 2082, 55856, 279, 1946, 2168, 323, 7023, 12305, 6168, 369, 279, 1790, 220, 23, 259, 76632, 13, 8886, 1917, 702, 220, 22, 15336, 13, 9258, 264, 3175, 8500, 315, 220, 20, 21, 25780, 320, 15, 12, 16, 15, 15, 15, 1817, 701, 14064, 279, 220, 23, 259, 76632, 94559, 13, 39565, 1172, 3550, 18663, 5109, 13, 12064, 770, 13, 151645, 198, 151644, 872, 198, 24669, 220, 16, 25, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 628, 279, 4158, 51489, 389, 279, 2115, 11968, 323, 2182, 279, 13753, 323, 4158, 51489, 389, 279, 1290, 11968, 151645, 198, 151644, 77091, 198, 19, 22, 19, 220, 19, 22, 24, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 22, 16, 220, 19, 22, 24, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 20, 23, 220, 19, 22, 24, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 20, 20, 220, 19, 22, 17, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 20, 20, 220, 19, 19, 19, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 20, 20, 220, 18, 24, 21, 220, 19, 19, 24, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 19, 24, 220, 18, 18, 20, 220, 19, 17, 17, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 17, 17, 220, 17, 23, 17, 220, 18, 24, 22, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 151645, 198]
```

**labels (433 tokens, first 226 masked)**:
```
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19, 22, 19, 220, 19, 22, 24, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 22, 16, 220, 19, 22, 24, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 20, 23, 220, 19, 22, 24, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 20, 20, 220, 19, 22, 17, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 20, 20, 220, 19, 19, 19, 220, 19, 21, 15, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 20, 20, 220, 18, 24, 21, 220, 19, 19, 24, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 19, 24, 220, 18, 18, 20, 220, 19, 17, 17, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 220, 19, 17, 17, 220, 17, 23, 17, 220, 18, 24, 22, 220, 21, 21, 24, 220, 18, 24, 16, 220, 21, 22, 19, 220, 15, 151645, 198]
```

**attention_mask (433 tokens, all 1s)**:
```
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

---

## 4. File Mapping

| Original | This Repo |
|----------|-----------|
| `rv_train/train.py` | `scripts/train.py` |
| `rv_train/models/qwen/model.py` | `src/rv_train/model.py` |
| `rv_train/data/libero_dataset.py` | `src/rv_train/dataset.py` |
| `libs/RoboVerse/roboverse/evals/` | `src/rv_eval/` |
| `eval/eval_libero.py` | `scripts/eval.py` |

---

## 5. Known Differences

- **Stats**: Original uses pre-computed; this repo samples at load time
